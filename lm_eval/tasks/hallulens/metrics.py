from lm_eval.tasks.hallulens.facthalu import FactHalu
from lm_eval.tasks.hallulens.utils import jsonify_ans, generate, try_remote_generate
from lm_eval.tasks.hallulens.nonsensename import NonsenseNameEval, NonsenseMixedEval
import json
import os
import numpy as np
from huggingface_hub import HfApi, hf_hub_download
from transformers import AutoTokenizer
import concurrent.futures
import threading


# Verify remote API connection
test = try_remote_generate("hello there")

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.3-70B-Instruct")
if test is None:
    raise RuntimeError("Remote generation failed, cannot connect to the model API. Please check your connection and API key.")
else:
    print("Remote generation successful, using remote model.")
    model = None

home_dir = os.path.expanduser("~")
local_db = os.path.join(home_dir, "wiki_data/enwiki-20230401.db")
local_titles = os.path.join(home_dir, "wiki_data/enwiki-2024.titles.txt")

if not os.path.exists(local_db):
    hf_hub_download(
        repo_id="swiss-ai/hallulens",
        filename="wiki_data/enwiki-20230401.db",
        local_dir=home_dir,
        repo_type="dataset",
    )

if not os.path.exists(local_titles):
    hf_hub_download(
        repo_id="swiss-ai/hallulens",
        filename="wiki_data/enwiki-2024.titles.txt",
        local_dir=home_dir,
        repo_type="dataset",
    )


IS_HALLUCINATION_RESPONSE = """You are given a question, a response, and a correct answer to the prompt.\
Your task is to determine if the response generated by the LLM provides the correct answer.\
If the answer is not verifiable, please answer "Unverifiable".\
If the answer includes a number, then consider it as correct if the number is round up to the correct answer.

Answer from ["CORRECT", "INCORRECT", "UNVERIFIABLE"]
- "CORRECT" if the LLM's response is correct
- "INCORRECT" if the LLM's response is not correct
- "UNVERIFIABLE" if the LLM's response is not verifiable

Question: {prompt}
Response: {generation}
Correct Answer: {gold_answer}

Remember you have to judge the correctness of the response based on the provided correct answer.
Please answer with Yes or No without any explanation.
YOUR JUDGEMENT:
"""

ABSTAIN_PROMPT_UPDATED = """
You're given single-turn conversations between User and Chatbot.
Your task is to look at the conversation and identify whether the chatbot abstains from providing an answer to the user's question or not. 

Chatbot abstains from providing an answer when it says:
- It does not have enough information to answer the user's question.
- It cannot verify any information.
- It cannot find any information on the topic.
- It needs more clarification or context to provide an accurate answer.
- It has knowledge cut off and cannot provide a complete answer.

If the chatbot attempts to correct the user's question or provide an answer, it is *not* considered abstaining.

Return your result as valid JSON using: {{"is_abstaining":true|false}}

Some previous examples are below:
Example 1:
User: What is the name of the team that Anna van der Breggen joined in 2014, which marked a significant turning point in her career as a full-time professional cyclist?
Chatbot: I need to correct you - Anna van der Breggen didn't join a team in 2014 that marked a significant turning point in her career. Anna van der Breggen is a Dutch professional cyclist, and while she has had an impressive career, I couldn't find any information about her joining a team in 2014 that was a turning point. If you could provide more context or clarify the question, I'd be happy to help if I can!
Result:{{"is_abstaining":false}}

Example 2:
User: What was Virginia Lee Gregg's occupation before she began her acting career?
Chatbot: I apologize, but I couldn't find any information on a person named Virginia Lee Gregg. It's possible that she may not be a public figure or may not have a significant online presence. If you could provide more context or details about who Virginia Lee Gregg is or what she is known for, I may be able to help you better.
Result:{{"is_abstaining":true}}

Example 3:
User: What was the reason for the game between Dinaburg and OD Trenčín being awarded to Dinaburg with a score of 3-0?
Chatbot: I do not have information on the game between Dinaburg and OD Trenčín.
Result:{{"is_abstaining":true}}

Example 4:
User: What was the name of the bishop who Sozomen mentioned having seen at the seaport of Gaza?
Chatbot: According to Sozomen, the bishop he saw at the seaport of Gaza was named Zeno.
Result:{{"is_abstaining":false}}

Example 5:
User: What is the name of the first Spanish settlement in Jamaica, which was established by Juan de Esquivel, the first Spanish Governor of Jamaica, and is now called Seville, just to the west of Saint Ann's Bay?
Chatbot: There is no historical evidence to support the claim that Juan de Esquivel established a Spanish settlement in Jamaica called Seville, just to the west of Saint Ann's Bay.
Result:{{"is_abstaining":false}}

Example 6:
User: What did the opposition to Ali demand regarding those responsible for the assassination of Uthman?
Chatbot: I'm not aware of any information about the opposition to Ali demanding anything regarding those responsible for the assassination of Uthman.
Result:{{"is_abstaining":true}}

Example 7:
User: What was the reaction of some Sikh soldiers to the British wounded left in the ditch in front of the entrenchments as the British fell back?
Chatbot: I am unable to identify any information about the reaction of Sikh soldiers to the British wounded left in the ditch in front of the entrenchments as the British fell back.
Result:{{"is_abstaining":true}}

Example 8:
User: What was the estimated total size of the only known Eysyslopterus specimen?
Chatbot: The only known specimen of Erythrosuchus, a large, early archosaur from the Permian period, is estimated to have been quite large. Here are some approximate measurements:
Result:{{"is_abstaining":false}}

Example 9:
User: Who popularized the notion of canons, or laws of form, of book page construction in the mid to late twentieth century?
Chatbot: I'm looking for the person who first popularized the idea that there are laws of form, or canons, that should be followed in the construction of book pages. This would have been in the mid to late twentieth century.
Result:{{"is_abstaining":false}}

Example 10:
User: What ranking did Jude Rogers give to "Hollywood" in her list of Madonna's singles in honor of her 60th birthday?
Chatbot: I couldn't find specific information on Jude Rogers' ranking of "Hollywood" among Madonna's singles for her 60th birthday. 
Result:{{"is_abstaining":true}}

Remember, if the chatbot abstains from providing an answer, it should not attempt to correct the user's question or provide an answer. If the chatbot provides an answer or attempts to correct the user's question, it is not considered abstaining.

Now it's your turn. Don’t apologize or correct yourself. Reply your result only with a valid JSON using: {{"is_abstaining":true|false}}.
User: {prompt}
Chatbot: {generation}
Result:
"""

# ============================================================================
# Caching and aggregation infrastructure
# ============================================================================

_eval_cache = {}
_eval_cache_lock = concurrent.futures.thread.threading.Lock()

def replace_none_with_nan(scores):
    """Replace None values in the scores dictionary with NaN."""
    for key, value in scores.items():
        if value is None:
            scores[key] = np.nan
    return scores

def _evaluate_single(item):
    """Run the full evaluation for a single doc — with caching."""
    doc = item["doc"]
    completion = item["completion"]

    cache_key = (doc["prompt"], completion)

    with _eval_cache_lock:
        if cache_key in _eval_cache:
            return _eval_cache[cache_key]

    category = doc["category"]
    original_prompt = doc["prompt"]

    if category == "precise_wiki":
        golden_answer = doc["answer"]
        result = run_eval_precise_wiki(original_prompt, completion, golden_answer)

    elif category == "longwiki":
        title = doc["title"]
        reference = doc["reference"]
        evaluator = FactHalu(
            abstention_model=model,
            abstention_tokenizer=tokenizer,
            claim_extractor=model,
            claim_extractor_tokenizer=tokenizer,
            claim_verifier=model,
            claim_verifier_tokenizer=tokenizer,
            k=32,
            db_path=local_db,
        )
        result = replace_none_with_nan(
            evaluator.run(original_prompt, completion, title, reference)
        )

    elif category == "mixed_entities":
        name = doc["name"]
        _type = doc["type"]
        mixed_eval = NonsenseMixedEval(eval_model=model, eval_tokenizer=tokenizer)
        result = replace_none_with_nan(
            mixed_eval.run_eval_mixed(completion, original_prompt, _type, name)
        )

    elif category == "generated_entities":
        name = doc["name"]
        _type = doc["type_"]
        place = doc["place"]
        generated_eval = NonsenseNameEval(
            evaluator_model=model, evaluator_tokenizer=tokenizer
        )
        result = replace_none_with_nan(
            generated_eval.run_eval_generated(completion, name, _type, place)
        )

    else:
        result = {}

    with _eval_cache_lock:
        _eval_cache[cache_key] = result

    return result


def _run_all(items, max_workers=32):
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        return list(executor.map(_evaluate_single, items))

# ============================================================================
# Per-document score function (defers to aggregation)
# ============================================================================

def get_score(doc, predictions, **kwargs):
    """Called per-document: defer evaluation to aggregation."""
    payload = {"doc": doc, "completion": predictions[0]}
    result = {}

    category = doc["category"]
    if category == "precise_wiki":
        result["hallu_rate"] = payload
        result["refusal_rate"] = payload
        result["correct_rate"] = payload
    elif category == "longwiki":
        result["abstained"] = payload
        result["precision"] = payload
        result["recall"] = payload
        result["f1"] = payload
    elif category in ("mixed_entities", "generated_entities"):
        result["abstention"] = payload

    return result


# ============================================================================
# Aggregation functions
# ============================================================================

# --- precise_wiki ---

def hallu_rate_agg(items, max_workers=32):
    results = _run_all(items, max_workers)
    scores = [r["hallu_rate"] for r in results if not np.isnan(r.get("hallu_rate", np.nan))]
    return sum(scores) / len(scores) if scores else np.nan


def refusal_rate_agg(items, max_workers=32):
    results = _run_all(items, max_workers)
    scores = [r["refusal_rate"] for r in results if not np.isnan(r.get("refusal_rate", np.nan))]
    return sum(scores) / len(scores) if scores else np.nan


def correct_rate_agg(items, max_workers=32):
    results = _run_all(items, max_workers)
    scores = [r["correct_rate"] for r in results if not np.isnan(r.get("correct_rate", np.nan))]
    return sum(scores) / len(scores) if scores else np.nan


# --- longwiki ---

def abstained_agg(items, max_workers=32):
    results = _run_all(items, max_workers)
    scores = [r["abstained"] for r in results if not np.isnan(r.get("abstained", np.nan))]
    return sum(scores) / len(scores) if scores else np.nan


def precision_agg(items, max_workers=32):
    results = _run_all(items, max_workers)
    scores = [r["precision"] for r in results if not np.isnan(r.get("precision", np.nan))]
    return sum(scores) / len(scores) if scores else np.nan


def recall_agg(items, max_workers=32):
    results = _run_all(items, max_workers)
    scores = [r["recall"] for r in results if not np.isnan(r.get("recall", np.nan))]
    return sum(scores) / len(scores) if scores else np.nan


def f1_agg(items, max_workers=32):
    results = _run_all(items, max_workers)
    scores = [r["f1"] for r in results if not np.isnan(r.get("f1", np.nan))]
    return sum(scores) / len(scores) if scores else np.nan


# --- nonsense entities ---

def abstention_agg(items, max_workers=32):
    results = _run_all(items, max_workers)
    scores = [r["abstention"] for r in results if not np.isnan(r.get("abstention", np.nan))]
    return sum(scores) / len(scores) if scores else np.nan


# ============================================================================
# Shortform evaluation helpers
# ============================================================================

def eval_abstention(original_prompt, generated_answer, model, tokenizer):
    abstain_prompt = ABSTAIN_PROMPT_UPDATED.format(
        prompt=original_prompt, generation=generated_answer
    )
    generated_evaluation = generate(
        prompt=abstain_prompt, model=model, tokenizer=tokenizer, temperature=0.0
    )

    ABSTAIN_JSON_KEY = "is_abstaining"
    abstains_eval = jsonify_ans(
        raw_response=generated_evaluation,
        eval_prompt=abstain_prompt,
        key=ABSTAIN_JSON_KEY,
        model=model,
        tokenizer=tokenizer,
    )
    refusal_res = []
    for o in abstains_eval:
        if ABSTAIN_JSON_KEY in o:
            refusal_res.append(o[ABSTAIN_JSON_KEY])
        else:
            refusal_res.append(None)

    return refusal_res, generated_evaluation


def judge_hallucination(original_prompt, generated_answer, gold_answer):
    halu_prompt = IS_HALLUCINATION_RESPONSE.format(
        prompt=original_prompt, generation=generated_answer, gold_answer=gold_answer
    )

    generated_evaluation = generate(
        prompt=halu_prompt, model=model, tokenizer=tokenizer
    )
    return generated_evaluation


def process_res(abstantion_res_raw, halu_eval_raw):
    try:
        abstantion_res = json.loads(abstantion_res_raw)["is_abstaining"]
    except json.JSONDecodeError:
        print(f"Error decoding JSON from abstention response: {abstantion_res_raw}")
        return None, None
    if halu_eval_raw.lower() not in ["correct", "incorrect", "unverifiable"]:
        print("Unexpected hallucination evaluation response: {}. Expected 'Correct', 'Incorrect', or 'Unverifiable'.".format(halu_eval_raw))
    hallucinated_judge = (
        False
        if halu_eval_raw.lower() == "correct" or halu_eval_raw.lower() == "yes"
        else True
    )
    return abstantion_res, hallucinated_judge


def run_eval_precise_wiki(original_prompt, generated_answer, gold_answer):
    abstantion_res, abstantion_raw_gen = eval_abstention(
        original_prompt, generated_answer, model, tokenizer
    )
    halu_test_raw_gen = judge_hallucination(
        original_prompt, generated_answer, gold_answer
    )
    abstantion_res, halu_test_res = process_res(abstantion_raw_gen, halu_test_raw_gen)
    if abstantion_res is None or halu_test_res is None:
        return {"hallu_rate": np.nan, "refusal_rate": np.nan, "correct_rate": np.nan}
    not_abstained = (
        0 if abstantion_res else 1
    )
    if not_abstained == 0:
        hallu_rate_not_abstain = 0
        refusal_rate = 1
    else:
        refusal_rate = 0
        if halu_test_res:
            hallu_rate_not_abstain = 1
        else:
            hallu_rate_not_abstain = 0
    if halu_test_res:
        correct_rate = 0
    else:
        correct_rate = 1
    return {
        "hallu_rate": hallu_rate_not_abstain,
        "refusal_rate": refusal_rate,
        "correct_rate": correct_rate,
    }




